{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75e9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import tensor as T\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import Trainer, ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM, ElectraForPreTraining, TrainingArguments\n",
    "\n",
    "SEED = 203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b54809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ElectraTokenizerFast.from_pretrained(f'google/electra-small-generator')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcddaf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"qqp\"\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbabdc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/eaj73/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d99aec82e64e75a82394cc430d26dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/363846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/eaj73/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-745117801ac214c5.arrow\n",
      "Loading cached processed dataset at /home/eaj73/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-628e29f19fd66073.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 159543 of the training set: {'question1': 'How would you feel if someone dated you out of pity?', 'question2': 'How do you know that you are on a first date with someone and not going out as friends?', 'label': 0, 'idx': 159543, 'input_ids': [101, 2129, 2052, 2017, 2514, 2065, 2619, 6052, 2017, 2041, 1997, 12063, 1029, 102, 2129, 2079, 2017, 2113, 2008, 2017, 2024, 2006, 1037, 2034, 3058, 2007, 2619, 1998, 2025, 2183, 2041, 2004, 2814, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "Sample 167936 of the training set: {'question1': 'What is the influence of Christianity in the Indian independence movement?', 'question2': 'What is the influence of Islam in the Indian independence movement?', 'label': 0, 'idx': 167936, 'input_ids': [101, 2054, 2003, 1996, 3747, 1997, 7988, 1999, 1996, 2796, 4336, 2929, 1029, 102, 2054, 2003, 1996, 3747, 1997, 7025, 1999, 1996, 2796, 4336, 2929, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "Sample 259252 of the training set: {'question1': 'How can I prevent shareholders from taking over my startup?', 'question2': 'What should you do when an Angel investor instead of signing a term sheet with a startup is directly emphasizing on signing the shareholder agreement.?', 'label': 0, 'idx': 259252, 'input_ids': [101, 2129, 2064, 1045, 4652, 15337, 2013, 2635, 2058, 2026, 22752, 1029, 102, 2054, 2323, 2017, 2079, 2043, 2019, 4850, 14316, 2612, 1997, 6608, 1037, 2744, 7123, 2007, 1037, 22752, 2003, 3495, 22671, 2006, 6608, 1996, 18668, 3820, 1012, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\n",
    "            \"glue\",\n",
    "            task_name,\n",
    "        )\n",
    "# Labels\n",
    "is_regression = task_name == \"stsb\"\n",
    "if not is_regression:\n",
    "    label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "    num_labels = len(label_list)\n",
    "else:\n",
    "    num_labels = 1\n",
    "\n",
    "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "\n",
    "# Preprocessing the raw_datasets\n",
    "def preprocess_function(examples):\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "    result[\"label\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"validation_matched\" if task_name == \"mnli\" else \"validation\"]\n",
    "test_dataset = raw_datasets[\"test_matched\" if task_name == \"mnli\" else \"test\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "metric = evaluate.load(\"glue\", task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a313ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ELECTRA Model, input: generator and discriminator\n",
    "## Joinly trained both models\n",
    "class ELECTRAModel(nn.Module):\n",
    "  def __init__(self, generator, discriminator, pad_token_id, dis_loss_weight= 50.0):\n",
    "    super().__init__()\n",
    "    self.generator, self.discriminator, self.pad_token_id = generator, discriminator, pad_token_id\n",
    "    self.gumbel_dist = torch.distributions.gumbel.Gumbel(torch.tensor(0., device='cuda:0'), torch.tensor(1., device='cuda:0'))\n",
    "    self.dis_loss_weight = dis_loss_weight\n",
    "    self.gen_loss_fc = nn.CrossEntropyLoss() #maybe Flatten\n",
    "    self.dis_loss_fc = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  def forward(self, masked_ids, attention_mask, token_type_ids, is_masked, labels):\n",
    "    # masked_ids: masked input ids (Tensor[int]): (B, L)\n",
    "    # attention_mask: attention_mask from data collator (Tensor[int]): (B, L)\n",
    "    # token_type_ids: token_type_ids of the tokenizer\n",
    "    # is_masked: whether the position is get masked (Tensor[boolean]): (B, L)\n",
    "    # labels: (Tensor[int]): (batch_size, seq_length) -100 for the unmasked\n",
    "\n",
    "    # Feed into generator\n",
    "    gen_logits = self.generator(masked_ids, attention_mask, token_type_ids).logits # (B, L, vocab size)\n",
    "    masked_gen_logits = gen_logits[is_masked,:] \n",
    "\n",
    "    with torch.no_grad():\n",
    "      # gumble arg-max to have tokenized predicted word\n",
    "      pred_toks = self.gumbel_softmax(masked_gen_logits)\n",
    "      # inputs for discriminator\n",
    "      gen_ids = masked_ids.clone() \n",
    "      gen_ids[is_masked] = pred_toks #(B, L)\n",
    "      # labels for discriminator\n",
    "      is_replaced = is_masked.clone() \n",
    "      is_replaced[is_masked] = (pred_toks != labels[is_masked]) #(B, L)\n",
    "\n",
    "    # Feed into discrminator\n",
    "    dis_logits = self.discriminator(gen_ids, attention_mask, token_type_ids).logits # (B, L, vocab size)\n",
    "\n",
    "    # Loss function of Electra\n",
    "    gen_loss = self.gen_loss_fc(masked_gen_logits.float(), labels[is_masked])\n",
    "    # Discriminator Loss\n",
    "    dis_logits = dis_logits.masked_select(attention_mask==1) \n",
    "    is_replaced = is_replaced.masked_select(attention_mask==1) \n",
    "    disc_loss = self.dis_loss_fc(dis_logits.float(), is_replaced.float())\n",
    "    loss = gen_loss + disc_loss * self.dis_loss_weight\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"masked_gen_logits\": masked_gen_logits, \n",
    "        \"gen_logits\": gen_logits,\n",
    "        \"dis_logits\": dis_logits\n",
    "        }\n",
    "      \n",
    "  def gumbel_softmax(self, logits):\n",
    "    return (logits.float() + self.gumbel_dist.sample(logits.shape)).argmax(dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ceff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the discriminator\n",
    "class ELECTRAClassificationModel(nn.Module):\n",
    "  def __init__(self, discriminator, classifier_dropout, num_labels):\n",
    "    super().__init__()\n",
    "    self.discriminator = discriminator\n",
    "    self.num_labels = num_labels\n",
    "    self.dropout = nn.Dropout(classifier_dropout)\n",
    "    self.classifier = nn.Linear(128, num_labels)\n",
    "    self.loss_fn = nn.CrossEntropyLoss() if not is_regression else nn.MSELoss()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "    dis_logits = self.discriminator(input_ids, attention_mask, token_type_ids).logits # (B, L, vocab size)\n",
    "\n",
    "    dis_logits = self.dropout(dis_logits)\n",
    "    logits = self.classifier(dis_logits)\n",
    "    \n",
    "    if is_regression:\n",
    "        loss = self.loss_fn(logits.squeeze(), labels.squeeze())\n",
    "    else:\n",
    "        loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"logits\": logits,\n",
    "        }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45f5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-small-discriminator')\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-small-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c22c6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ElectraForMaskedLM(gen_config)\n",
    "discriminator = ElectraForPreTraining(disc_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386806b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = ELECTRAModel(generator, discriminator, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b06b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.load_state_dict(torch.load(\"./pretrained/pytorch_model.bin\"))\n",
    "model = ELECTRAClassificationModel(pretrained_model.discriminator, 0.1, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65473d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "    if len(result) > 1:\n",
    "        result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a1145ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./models\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-4,\n",
    "    logging_dir= \"./logs\",\n",
    "    report_to= \"wandb\",\n",
    "    remove_unused_columns=True,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=False,\n",
    "    weight_decay=0,\n",
    "    save_steps=1000,\n",
    "    adam_epsilon=1e-6,\n",
    "#     max_steps=62500,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0770719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eaj73/.conda/envs/colbert/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkliang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eaj73/Electra_replicate/wandb/run-20230407_180420-pd94dd66</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kliang/huggingface/runs/pd94dd66' target=\"_blank\">quiet-planet-9</a></strong> to <a href='https://wandb.ai/kliang/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kliang/huggingface' target=\"_blank\">https://wandb.ai/kliang/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kliang/huggingface/runs/pd94dd66' target=\"_blank\">https://wandb.ai/kliang/huggingface/runs/pd94dd66</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2061' max='34113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2061/34113 02:32 < 39:34, 13.50 it/s, Epoch 0.18/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.658600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.660400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e2f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"*** Evaluate ***\")\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "print(\"*** Test ***\")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
