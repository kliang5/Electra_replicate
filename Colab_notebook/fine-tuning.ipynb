{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75e9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import tensor as T\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import Trainer, ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM, ElectraForPreTraining, TrainingArguments\n",
    "\n",
    "SEED = 203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b54809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ElectraTokenizerFast.from_pretrained(f'google/electra-small-generator')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcddaf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"qqp\"\n",
    "task_to_keys = {\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\n",
    "            \"glue\",\n",
    "            task_name,\n",
    "        )\n",
    "# Labels\n",
    "is_regression = task_name == \"stsb\"\n",
    "if not is_regression:\n",
    "    label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "    num_labels = len(label_list)\n",
    "else:\n",
    "    num_labels = 1\n",
    "\n",
    "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "\n",
    "# Preprocessing the raw_datasets\n",
    "def preprocess_function(examples):\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "    result[\"label\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"validation_matched\" if task_name == \"mnli\" else \"validation\"]\n",
    "test_dataset = raw_datasets[\"test_matched\" if task_name == \"mnli\" else \"test\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "metric = evaluate.load(\"glue\", task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a313ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ELECTRA Model, input: generator and discriminator\n",
    "## Joinly trained both models\n",
    "class ELECTRAModel(nn.Module):\n",
    "  def __init__(self, generator, discriminator, pad_token_id, dis_loss_weight= 50.0):\n",
    "    super().__init__()\n",
    "    self.generator, self.discriminator, self.pad_token_id = generator, discriminator, pad_token_id\n",
    "    self.gumbel_dist = torch.distributions.gumbel.Gumbel(torch.tensor(0., device='cuda:0'), torch.tensor(1., device='cuda:0'))\n",
    "    self.dis_loss_weight = dis_loss_weight\n",
    "    self.gen_loss_fc = nn.CrossEntropyLoss() #maybe Flatten\n",
    "    self.dis_loss_fc = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  def forward(self, masked_ids, attention_mask, token_type_ids, is_masked, labels):\n",
    "    # masked_ids: masked input ids (Tensor[int]): (B, L)\n",
    "    # attention_mask: attention_mask from data collator (Tensor[int]): (B, L)\n",
    "    # token_type_ids: token_type_ids of the tokenizer\n",
    "    # is_masked: whether the position is get masked (Tensor[boolean]): (B, L)\n",
    "    # labels: (Tensor[int]): (batch_size, seq_length) -100 for the unmasked\n",
    "\n",
    "    # Feed into generator\n",
    "    gen_logits = self.generator(masked_ids, attention_mask, token_type_ids).logits # (B, L, vocab size)\n",
    "    masked_gen_logits = gen_logits[is_masked,:] \n",
    "\n",
    "    with torch.no_grad():\n",
    "      # gumble arg-max to have tokenized predicted word\n",
    "      pred_toks = self.gumbel_softmax(masked_gen_logits)\n",
    "      # inputs for discriminator\n",
    "      gen_ids = masked_ids.clone() \n",
    "      gen_ids[is_masked] = pred_toks #(B, L)\n",
    "      # labels for discriminator\n",
    "      is_replaced = is_masked.clone() \n",
    "      is_replaced[is_masked] = (pred_toks != labels[is_masked]) #(B, L)\n",
    "\n",
    "    # Feed into discrminator\n",
    "    dis_logits = self.discriminator(gen_ids, attention_mask, token_type_ids).logits # (B, L, vocab size)\n",
    "\n",
    "    # Loss function of Electra\n",
    "    gen_loss = self.gen_loss_fc(masked_gen_logits.float(), labels[is_masked])\n",
    "    # Discriminator Loss\n",
    "    dis_logits = dis_logits.masked_select(attention_mask==1) \n",
    "    is_replaced = is_replaced.masked_select(attention_mask==1) \n",
    "    disc_loss = self.dis_loss_fc(dis_logits.float(), is_replaced.float())\n",
    "    loss = gen_loss + disc_loss * self.dis_loss_weight\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"masked_gen_logits\": masked_gen_logits, \n",
    "        \"gen_logits\": gen_logits,\n",
    "        \"dis_logits\": dis_logits\n",
    "        }\n",
    "      \n",
    "  def gumbel_softmax(self, logits):\n",
    "    return (logits.float() + self.gumbel_dist.sample(logits.shape)).argmax(dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ceff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the discriminator\n",
    "class ELECTRAClassificationModel(nn.Module):\n",
    "  def __init__(self, discriminator, classifier_dropout, num_labels):\n",
    "    super().__init__()\n",
    "    self.discriminator = discriminator\n",
    "    self.num_labels = num_labels\n",
    "    self.dropout = nn.Dropout(classifier_dropout)\n",
    "    self.classifier = nn.Linear(128, num_labels)\n",
    "    self.loss_fn = nn.CrossEntropyLoss() if not is_regression else nn.MSELoss()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "    dis_logits = self.discriminator(input_ids, attention_mask, token_type_ids).logits # (B, L, vocab size)\n",
    "\n",
    "    dis_logits = self.dropout(dis_logits)\n",
    "    logits = self.classifier(dis_logits)\n",
    "    \n",
    "    if is_regression:\n",
    "        loss = self.loss_fn(logits.squeeze(), labels.squeeze())\n",
    "    else:\n",
    "        loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"logits\": logits,\n",
    "        }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45f5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-small-discriminator')\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-small-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c22c6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ElectraForMaskedLM(gen_config)\n",
    "discriminator = ElectraForPreTraining(disc_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386806b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = ELECTRAModel(generator, discriminator, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b06b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.load_state_dict(torch.load(\"./pretrained/pytorch_model.bin\"))\n",
    "model = ELECTRAClassificationModel(pretrained_model.discriminator, 0.1, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65473d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "    if len(result) > 1:\n",
    "        result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a1145ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./models\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-4,\n",
    "    logging_dir= \"./logs\",\n",
    "    report_to= \"wandb\",\n",
    "    remove_unused_columns=True,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=False,\n",
    "    weight_decay=0,\n",
    "    save_steps=1000,\n",
    "    adam_epsilon=1e-6,\n",
    "#     max_steps=62500,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa982451",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e2f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"*** Evaluate ***\")\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "print(\"*** Test ***\")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
